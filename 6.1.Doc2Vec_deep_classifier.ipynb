{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification\n",
    "### 1. Merge one hot encoded genres to lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-898b77bd4b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../dataset/Lyrics_en_artists_clean_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Band'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../dataset/Lyrics_en_artists_clean_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Band'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \"\"\"\n\u001b[0;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/Lyrics_en_artists_clean_train.csv', index_col=['Band'])\n",
    "df_val = pd.read_csv('../dataset/Lyrics_en_artists_clean_test.csv', index_col=['Band'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label exploration  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "y_train = df.drop(df.columns[-10:].append('Lyrics'), axis=1)\n",
    "y_test = df_val.drop(df.columns[-10:].append('Lyrics'), axis=1)\n",
    "test_lyrics = df_val['Lyrics']\n",
    "del df\n",
    "del df_val\n",
    "#pd.scatter_matrix(df, alpha = 0.3, figsize = (14,8), diagonal = 'kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Doc2Vec Deep learning LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "doc2vec = Doc2Vec.load(\"../dataset/doc2vec\")\n",
    "doc_vectors = doc2vec.wv\n",
    "print(\"Number of word vectors: {}\".format(len(doc_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['s', 'chorus'])\n",
    "\n",
    "# python -m spacy download en\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "def transform_lyric(lyric):\n",
    "    lyric = simple_preprocess(str(lyric), deacc=True)\n",
    "\n",
    "    new_lyric = list()\n",
    "    for word in lyric:\n",
    "        if(word not in stop_words):\n",
    "            new_lyric.append(word)\n",
    "    lyric = new_lyric\n",
    "\n",
    "\n",
    "    lemma_lyric = list()\n",
    "    doc = nlp(\" \".join(lyric)) \n",
    "    lemma_lyric = list()\n",
    "    for token in doc:\n",
    "        if(token.pos_ in allowed_postags):\n",
    "            lemma_lyric.append(token.lemma_)\n",
    "    lyric = lemma_lyric\n",
    "    return lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Text_INPUT_DIM = 100\n",
    "train_size = y_train.shape[0] \n",
    "test_size = y_test.shape[0] \n",
    "text_train_arrays = np.zeros((train_size, Text_INPUT_DIM))\n",
    "text_test_arrays = np.zeros((test_size, Text_INPUT_DIM))\n",
    "\n",
    "for i in range(train_size):\n",
    "    text_train_arrays[i] = doc2vec.docvecs[i]\n",
    "    \n",
    "for i in range(test_size):\n",
    "    test_lyrics[i] = transform_lyric(test_lyrics[i])\n",
    "    text_test_arrays[i] = doc2vec.infer_vector(test_lyrics[i])\n",
    "#text_test_arrays = doc2vec.infer_vector(test_lyrics)\n",
    "#del test_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(input_dim=train_size, output_dim=100))\n",
    "model_conv.add(Dropout(0.2))\n",
    "model_conv.add(Conv1D(64, 5, activation='relu'))\n",
    "model_conv.add(MaxPooling1D(pool_size=4))\n",
    "model_conv.add(LSTM(100))\n",
    "model_conv.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "model_conv.summary()\n",
    "\n",
    "estimator = model_conv.fit(text_train_arrays, y_train, validation_data=(text_test_arrays,y_test), epochs = 2, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/Lyrics_en_artists_clean_train.csv', index_col=['Band'])\n",
    "train_lyrics = df['Lyrics']\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bryce Fox\n",
    "#Lucy\n",
    "#genres: \"indie poptimism\",\"modern alternative rock\",\"modern rock\"\n",
    "\n",
    "lyric = \"So bring your vibe over here here here \\\n",
    "Throw it up in the air air air, oh \\\n",
    "I run my hands through your hair hair hair \\\n",
    "And give you that love cuz you're oh so rare \\\n",
    "You look better over here then take you do over there \\\n",
    "And I don't wanna share \\\n",
    " \\\n",
    "You don't let a good thing go to waste \\\n",
    "So I took the lipstick off that face \\\n",
    "I don't wanna share, yeah \\\n",
    " \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    " \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet \\\n",
    "She said she ain't Lucy \\\n",
    "I'll fix you up something real real tall \\\n",
    "It's looking up, we should take it all off \\\n",
    "Yeah \\\n",
    "You remind me why the stars don't fall \\\n",
    "I swear I'll love you like your last name's Ball \\\n",
    "I'll take you to the lair if we make it past the stairs \\\n",
    "I don't wanna share, yeah \\\n",
    " \\\n",
    "You don't let a good thing go to waste \\\n",
    "So I took the lipstick off that face \\\n",
    "I don't wanna share, yeah \\\n",
    " \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    " \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet \\\n",
    "She said she ain't Lucy \\\n",
    " \\\n",
    "Said I'm just tryna get to know ya \\\n",
    "So bite your tongue before I bite it for ya \\\n",
    "Yeah \\\n",
    "Said I'm just tryna get to know ya \\\n",
    "So bite your tongue before I bite it for ya \\\n",
    " \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    "If your name was Lucy, I'd put Lucy in her Lucifer \\\n",
    "My god, what in the devil, bring the animal right out of her \\\n",
    " \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet \\\n",
    "(Said I'm just tryna get to know ya) \\\n",
    "(So bite your tongue before I bite it for ya) \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet \\\n",
    "(Said I'm just tryna get to know ya) \\\n",
    "She said she ain't Lucy \\\n",
    "I said not yet\" \n",
    "\n",
    "lyric = transform_lyric(lyric)\n",
    "print(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros((1, Text_INPUT_DIM))\n",
    "vector[0] = doc2vec.infer_vector(lyric)\n",
    "\n",
    "sims = doc2vec.docvecs.most_similar([vector[0]], topn=2)\n",
    "for sim in sims:\n",
    "    print(train_lyrics[sim[0]], sim[1], sim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_conv.predict(vector)\n",
    "idx = (-y_pred[0]).argsort()[:10]\n",
    "dict(zip(y_train.columns.values[idx],y_pred[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = \"I hear you talkin' shit, bro you think you're the heat \\\n",
    "Please bow down to defeat you're barely mince meat \\\n",
    "Stop with the street talk, and start to do the street, walk \\\n",
    "Lock yourself in and tell me this, how you gonna battle with this sick shit that I spit bitch \\\n",
    "See you still flappin' your jaws, prepare to be thrown in the ocean \\\n",
    "In the middle with jaws \\\n",
    "Or prefer to be served and severed with claws, knock on the doors Of absolute death you may \\\n",
    "But please be ready for the automatic failures you may make \\\n",
    "So take this and wait for another dismembering, remembering \\\n",
    "The tethering of your dream and how it was minced to pieces \\\n",
    "Believe in yourself please, to save you now you'll be needin' \\\n",
    "Help from Jesus \\\n",
    "You think I'm copy, pastin' please, tell me how you're actin' up \\\n",
    "Better be quite or Imma bout to bitch smack your mouth shut \\\n",
    "For good, yeah man talk the hood talk better go back and walk the hood walk you ain't had the taste of a real G \\\n",
    "Get back down to your level and prepare to be beat \\\n",
    "Writin' on your sheet, like you're deep please bitch your weak \\\n",
    "You ain't on my level or a level at all, take a vacation \\\n",
    "Up the damn wall and fall \\\n",
    "Humpty Dumpty, I must be rusty but you better knock the shit Down if you goin' against me \\\n",
    "I'm hard to beat, you're hardly cheese compared to these beats And this emcee \\\n",
    "You ready, alright, so come on down, I bet you to do it \\\n",
    "Let's see if you got the balls or if you're just truant \\\n",
    "To me, yeah let's keep an eye, on your ass as the heat intensifies \\\n",
    "Talk to me in a smart tone, prepare to meet your demise \\\n",
    "Throw you, a hundred million miles is what im finna do \\\n",
    "Call me Son Goku \\\n",
    "I already told you, what I'm gonna do, finna plow through \\\n",
    "Yeah man try to tackle a topic more intense then mine \\\n",
    "I always rap from the heart, make the fake rappers sigh \\\n",
    "But I really really don't give at all a flying fuck \\\n",
    "My chance of making this shit is the same as seein' a flyin' truck \\\n",
    "Or it could all depend on some serious luck \\\n",
    "Maybe I'll release a track that is heard my millions \\\n",
    "Make it big and to the top and make billions \\\n",
    "But that's a dream for another day, I'm here to take you down \\\n",
    "Don't care what you say \\\n",
    "Freestylin' this rap shit to the grave \\\n",
    "Rap battlin' bitches in my spare time for loose change \\\n",
    "Please, all of you fake ass rappers are plan \\\n",
    "Make like like a plane and take the next trip away \\\n",
    "Far away, from here I don't need to here your shit anymore \\\n",
    "I'm tired of it now, next place to be, will be a fuckin' morgue \\\n",
    "But let me just let you all know this, I'm sick of it, all of it \\\n",
    "So listen next time I write another track \\\n",
    "Listen to the facts I spit, and find it through my old wack rap crap \\\n",
    "I know I've said some shit, that know I demise \\\n",
    "But just look at my own and, your own life \\\n",
    "But this time, through my eyes \\\n",
    "I got a couple little bitches that need a little call out \\\n",
    "But I'll reside, got more and more, don't get me wrong \\\n",
    "Now get the fuck back home and call your mom \\\n",
    "Continue to rap please, just leave me alone \\\n",
    "I'm done hearing wack disses like this \\\n",
    "I'm sick of this, listening to this petty as shit \\\n",
    "Bitch\"\n",
    "lyric = transform_lyric(lyric)\n",
    "print(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros((1, Text_INPUT_DIM))\n",
    "vector[0] = doc2vec.infer_vector(lyric)\n",
    "\n",
    "sims = doc2vec.docvecs.most_similar([vector[0]], topn=2)\n",
    "for sim in sims:\n",
    "    print(train_lyrics[sim[0]], sim[1], sim[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_conv.predict(vector)\n",
    "idx = (-y_pred[0]).argsort()[:10]\n",
    "dict(zip(y_train.columns.values[idx],y_pred[0][idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
